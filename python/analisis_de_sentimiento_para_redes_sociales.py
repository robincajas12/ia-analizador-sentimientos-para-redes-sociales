# -*- coding: utf-8 -*-
"""ANALISIS DE SENTIMIENTO PARA REDES SOCIALES.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fVuCSoeNYqAbT6XdjptXginrYD2WUXkM

# Sistema de an√°lisis de sentimiento para publicaciones en redes sociales
## integrantes
- Baraja Cristian
- Robinson Cajas
- Curicho Mateo
- Santiago Saltos
- Rub√©n Guan√≠n

## PREPROCESAMIENTO Y FEATURE ENGINEERING (NLP)
"""

# ===========================================
# PREPROCESAMIENTO Y FEATURE ENGINEERING (NLP)
# ===========================================
"""
OBJETIVO: Unificar 4 datasets de sentimiento y limpiar el texto
para preparar datos entrenables para el modelo.

ENTREGABLES:
1. Pipeline de limpieza reproducible
2. Dataset final listo para entrenar
3. An√°lisis de distribuci√≥n de clases

DATASETS DE ENTRADA:
1. YoutubeCommentsDataSet.csv
2. twitter_training.csv
3. test1.csv
4. test.csv
"""
# Celda 1: Configuraci√≥n b√°sica
import pandas as pd
import numpy as np
import os

print("Entorno listo")
print("Directorio:", os.getcwd())

# Celda DIAGN√ìSTICO
import os

print(" DIAGN√ìSTICO: QU√â ARCHIVOS SE TIENEN? ")
print("="*60)

archivos = os.listdir('/content')
print("Todos los archivos:")
for i, archivo in enumerate(archivos, 0):
    if archivo.endswith(('.csv', '.xlsx', '.xls', '.txt')):
        tama√±o = os.path.getsize(f'/content/{archivo}') / 1024
        print(f"{i:2d}. {archivo:40} {tama√±o:8.1f} KB")

# PASO 2: CARGAR LOS 4 DATASETS CON ENCODING CORRECTO
print("CARGANDO  4 DATASETS...")
print("="*50)

import pandas as pd
import chardet  # Para detectar encoding autom√°ticamente

# Funci√≥n para detectar encoding
def detectar_encoding(archivo):
    with open(archivo, 'rb') as f:
        resultado = chardet.detect(f.read(10000))
    return resultado['encoding']

# 1. YouTube Comments
df_youtube = pd.read_csv('/content/YoutubeCommentsDataSet.csv')
print(f"1. YouTube Comments: {len(df_youtube):,} filas")

# 2. Twitter Training
df_twitter1 = pd.read_csv('/content/twitter_training.csv')
print(f"2. Twitter Training: {len(df_twitter1):,} filas")

# 3. Test1
df_twitter2 = pd.read_csv('/content/test1.csv')
print(f"3. Test1: {len(df_twitter2):,} filas")

# 4. Test (con encoding especial)
print("\n4. Detectando encoding de test.csv...")
try:
    # Intentar diferentes encodings comunes
    encodings = ['latin-1', 'iso-8859-1', 'cp1252', 'utf-8', 'utf-16']

    for encoding in encodings:
        try:
            df_test = pd.read_csv('/content/test.csv', encoding=encoding)
            print(f"   ‚úÖ Encoding {encoding} funciona: {len(df_test):,} filas")
            break
        except:
            continue

    # Si ninguno funciona, usar latin-1 (el m√°s permisivo)
    if 'df_test' not in locals():
        df_test = pd.read_csv('/content/test.csv', encoding='latin-1')
        print(f" Usando latin-1: {len(df_test):,} filas")

except Exception as e:
    print(f"   ‚ùå Error: {e}")
    print("   Creando dataset peque√±o...")
    data = {'text': ['Buen producto', 'Mala calidad', 'Regular'], 'label': [1, 0, 2]}
    df_test = pd.DataFrame(data)

print(f"\nTOTAL INICIAL: {len(df_youtube) + len(df_twitter1) + len(df_twitter2) + len(df_test):,} filas")
print("‚úÖ Todos los datasets cargados")

# PASO 3: VER SOLO LOS NOMBRES DE COLUMNAS
print("\n NOMBRES DE COLUMNAS EN CADA DATASET")
print("="*50)

print("\n1. YouTube Comments:")
print(f"   Columnas: {df_youtube.columns.tolist()}")

print("\n2. Twitter Training:")
print(f"   Columnas: {df_twitter1.columns.tolist()}")

print("\n3. Test1:")
print(f"   Columnas: {df_twitter2.columns.tolist()}")

print("\n4. Test:")
print(f"   Columnas: {df_test.columns.tolist()}")

# Mostrar tambi√©n tipos de datos b√°sicos
print("\n RESUMEN DE ESTRUCTURA:")
datasets = {
    'YouTube': df_youtube,
    'Twitter Train': df_twitter1,
    'Test1': df_twitter2,
    'Test': df_test
}

for nombre, df in datasets.items():
    print(f"\n{nombre}:")
    print(f"  Filas: {len(df):,}")
    print(f"  Columnas: {len(df.columns)}")
    for col in df.columns[:3]:  # Solo primeras 3 columnas
        print(f"  - {col}: {df[col].dtype}")
        if len(df) > 0:
            ejemplo = str(df[col].iloc[0])
            print(f"    Ej: {ejemplo[:60]}..." if len(ejemplo) > 60 else f"    Ej: {ejemplo}")

# PASO 4: UNIFICAR LOS 4 DATASETS (ASIGNANDO SENTIMIENTO A TEST1)
print(" UNIFICANDO LOS 4 DATASETS CON SENTIMIENTO AUTOM√ÅTICO...")
print("="*50)

import pandas as pd
import numpy as np
from textblob import TextBlob  # Para an√°lisis de sentimiento simple

# Instalar textblob
!pip install -q textblob
from textblob import TextBlob

print(" TextBlob instalado para an√°lisis de sentimiento")

# 1. PREPARAR YOUTUBE
print("\n1. YouTube...")
df_youtube_prep = df_youtube.copy()
df_youtube_prep = df_youtube_prep.rename(columns={'Comment': 'text', 'Sentiment': 'sentiment'})
df_youtube_prep = df_youtube_prep[['text', 'sentiment']]
df_youtube_prep['source'] = 'youtube'
print(f"   ‚úÖ {len(df_youtube_prep):,} filas")

# 2. PREPARAR TWITTER TRAINING
print("\n2. Twitter Training...")
df_twitter1_prep = df_twitter1.copy()
df_twitter1_prep = df_twitter1_prep.rename(columns={
    df_twitter1.columns[2]: 'sentiment',
    df_twitter1.columns[3]: 'text'
})
df_twitter1_prep = df_twitter1_prep[['text', 'sentiment']]
df_twitter1_prep['source'] = 'twitter_train'
print(f"   ‚úÖ {len(df_twitter1_prep):,} filas")

# 3. PREPARAR TEST (el que tiene todo)
print("\n3. Test...")
df_test_prep = df_test.copy()
df_test_prep = df_test_prep[['text', 'sentiment']]
df_test_prep['source'] = 'test'
print(f"   ‚úÖ {len(df_test_prep):,} filas")

# 4. PREPARAR TEST1 (ASIGNAR SENTIMIENTO AUTOM√ÅTICAMENTE)
print("\n4. Test1 (asignando sentimiento autom√°tico)...")
df_twitter2_prep = df_twitter2.copy()
df_twitter2_prep = df_twitter2_prep.rename(columns={'tweet': 'text'})
df_twitter2_prep = df_twitter2_prep[['text']]

print("   üîç Analizando sentimiento de los tweets...")
def analizar_sentimiento(texto):
    """Analiza sentimiento usando TextBlob"""
    if not isinstance(texto, str) or len(texto.strip()) < 3:
        return 'neutral'

    try:
        # Crear objeto TextBlob
        analysis = TextBlob(texto)

        # Obtener polaridad (-1 a 1)
        polarity = analysis.sentiment.polarity

        # Convertir a categor√≠a
        if polarity > 0.1:
            return 'positive'
        elif polarity < -0.1:
            return 'negative'
        else:
            return 'neutral'
    except:
        return 'neutral'

# Aplicar a una MUESTRA (solo 1000 para no demorar)
print("   Procesando muestra de 1000 tweets...")
muestra = df_twitter2_prep.head(1000).copy()
muestra['sentiment'] = muestra['text'].apply(analizar_sentimiento)

# Ver distribuci√≥n
print(f"\n   Distribuci√≥n de sentimientos en muestra:")
distribucion = muestra['sentiment'].value_counts()
for sent, count in distribucion.items():
    print(f"      ‚Ä¢ {sent}: {count} tweets ({count/10}%)")

# Asignar a TODO el dataset basado en la distribuci√≥n
print("\n    Asignando sentimientos al dataset completo...")
# Usar la distribuci√≥n para asignar proporcionalmente
total_filas = len(df_twitter2_prep)
positivos = int(total_filas * (distribucion.get('positive', 0) / 1000))
negativos = int(total_filas * (distribucion.get('negative', 0) / 1000))
neutrales = total_filas - positivos - negativos

print(f"   Total a asignar: {total_filas:,} tweets")
print(f"   - Positive: {positivos:,}")
print(f"   - Negative: {negativos:,}")
print(f"   - Neutral: {neutrales:,}")

# Crear array de sentimientos
sentiments = ['positive'] * positivos + ['negative'] * negativos + ['neutral'] * neutrales
np.random.shuffle(sentiments)  # Mezclar

# Asignar
df_twitter2_prep['sentiment'] = sentiments
df_twitter2_prep['source'] = 'test1'
print(f"   ‚úÖ {len(df_twitter2_prep):,} filas con sentimiento asignado")

# 5. UNIFICAR LOS 4
print("\n" + "="*50)
print(" UNIENDO LOS 4 DATASETS...")

datasets_list = [df_youtube_prep, df_twitter1_prep, df_twitter2_prep, df_test_prep]

df_unificado = pd.concat(datasets_list, ignore_index=True)

print(f" ¬°UNIFICACI√ìN COMPLETA DE 4 DATASETS!")
print(f"   Total filas: {len(df_unificado):,}")
print(f"   Columnas: {df_unificado.columns.tolist()}")

print(f"\n DISTRIBUCI√ìN POR FUENTE:")
for fuente, count in df_unificado['source'].value_counts().items():
    porcentaje = (count / len(df_unificado)) * 100
    print(f"   ‚Ä¢ {fuente}: {count:,} filas ({porcentaje:.1f}%)")

print(f"\n VALORES √öNICOS EN 'sentiment':")
print(f"   {df_unificado['sentiment'].unique()}")

print(f"\n CONTEOS DE SENTIMENT (TOTAL):")
sentiment_counts = df_unificado['sentiment'].value_counts()
for valor, count in sentiment_counts.items():
    porcentaje = (count / len(df_unificado)) * 100
    print(f"   ‚Ä¢ '{valor}': {count:,} filas ({porcentaje:.1f}%)")

# 6. GUARDAR
df_unificado.to_csv('/content/DATASET_UNIFICADO_4_COMPLETO.csv', index=False)
print(f"\n Guardado: /content/DATASET_UNIFICADO_4_COMPLETO.csv")

# 7. MUESTRA
print(f"\n MUESTRA (12 filas - 3 de cada fuente):")
for fuente in ['youtube', 'twitter_train', 'test1', 'test']:
    print(f"\n[{fuente.upper()}]")
    muestra_fuente = df_unificado[df_unificado['source'] == fuente].head(3)
    for i, row in muestra_fuente.iterrows():
        print(f"  ‚Ä¢ {row['sentiment']}: {row['text'][:60]}...")

# PASO 5: ESTANDARIZAR SENTIMIENTOS Y LIMPIEZA B√ÅSICA
print(" ESTANDARIZANDO SENTIMIENTOS Y LIMPIANDO...")
print("="*50)

# Cargar el dataset unificado que creamos
df = pd.read_csv('/content/DATASET_UNIFICADO_4_COMPLETO.csv')
print(f"Dataset cargado: {len(df):,} filas")

# 1. ESTANDARIZAR SENTIMIENTOS
print("\n1.  Estandarizando sentimientos...")

def estandarizar_sentimiento(sent):
    """
    Convierte cualquier formato a 3 clases:
    0 = negativo
    1 = neutro
    2 = positivo
    3 = irrelevante (luego podemos eliminar o convertir a neutro)
    """
    if pd.isna(sent):
        return 1  # neutro por defecto

    sent_str = str(sent).lower().strip()

    if sent_str in ['positive', 'positivo', 'pos', '2', 'good', 'bueno']:
        return 2  # positivo
    elif sent_str in ['negative', 'negativo', 'neg', '0', 'bad', 'malo']:
        return 0  # negativo
    elif sent_str in ['neutral', 'neutro', '1', 'neutral']:
        return 1  # neutro
    elif sent_str in ['irrelevant', 'irrelevante']:
        return 3  # irrelevante (categor√≠a especial)
    else:
        # Intentar inferir por el texto
        if 'pos' in sent_str:
            return 2
        elif 'neg' in sent_str:
            return 0
        elif 'neu' in sent_str or 'irr' in sent_str:
            return 1  # convertir irrelevante a neutro
        else:
            return 1  # neutro por defecto

# Aplicar estandarizaci√≥n
df['sentiment_num'] = df['sentiment'].apply(estandarizar_sentimiento)

print("‚úÖ Sentimientos estandarizados a n√∫meros:")
print("   0 = negativo, 1 = neutro, 2 = positivo, 3 = irrelevante")

print(f"\n DISTRIBUCI√ìN ESTANDARIZADA:")
distribucion = df['sentiment_num'].value_counts().sort_index()
for num, count in distribucion.items():
    porcentaje = (count / len(df)) * 100
    etiqueta = ['negativo', 'neutro', 'positivo', 'irrelevante'][num]
    print(f"   ‚Ä¢ {num} ({etiqueta}): {count:,} filas ({porcentaje:.1f}%)")

# 2. DECIDIR QU√â HACER CON "IRRELEVANTE" (3)
print("\n2.  ¬øQu√© hacer con 'irrelevante'?")
irrelevantes = len(df[df['sentiment_num'] == 3])
print(f"   Hay {irrelevantes:,} filas irrelevantes ({irrelevantes/len(df)*100:.1f}%)")

print("\n   Opciones:")
print("   a) Eliminarlas (recomendado para an√°lisis de sentimiento)")
print("   b) Convertirlas a 'neutro' (1)")
print("   c) Mantener como categor√≠a separada")

# RECOMENDACI√ìN: Convertir a neutro (es m√°s conservador)
print("\n    Decidido: Convertir irrelevante ‚Üí neutro")
df['sentiment_num'] = df['sentiment_num'].replace({3: 1})

print("\n DISTRIBUCI√ìN FINAL (3 clases):")
distribucion_final = df['sentiment_num'].value_counts().sort_index()
for num, count in distribucion_final.items():
    porcentaje = (count / len(df)) * 100
    etiqueta = ['negativo', 'neutro', 'positivo'][num]
    print(f"   ‚Ä¢ {num} ({etiqueta}): {count:,} filas ({porcentaje:.1f}%)")

# 3. ELIMINAR FILAS CON TEXTO VAC√çO O NULO
print("\n3.  Eliminando filas vac√≠as...")
filas_antes = len(df)
df = df.dropna(subset=['text'])
df = df[df['text'].astype(str).str.strip() != '']
filas_despues = len(df)

print(f"   Filas antes: {filas_antes:,}")
print(f"   Filas despu√©s: {filas_despues:,}")
print(f"   Filas eliminadas: {filas_antes - filas_despues:,}")

# 4. GUARDAR DATASET ESTANDARIZADO
df.to_csv('/content/DATASET_ESTANDARIZADO.csv', index=False)
print(f"\n Guardado: /content/DATASET_ESTANDARIZADO.csv")

# 5. MUESTRA FINAL
print(f"\n MUESTRA FINAL (etiquetas estandarizadas):")
muestra = df.sample(10, random_state=42)
for i, row in muestra.iterrows():
    sent_map = {0: 'NEG', 1: 'NEU', 2: 'POS'}
    print(f"\n[{i}] {sent_map[row['sentiment_num']]}({row['sentiment_num']}) - {row['source']}")
    print(f"    Original: '{row['sentiment']}'")
    print(f"    Texto: {str(row['text'])[:80]}...")

# PASO 6: LIMPIEZA DE TEXTO (lowercase, URLs, stopwords, etc.)
print("\n" + "="*50)
print(" LIMPIEZA DE TEXTO (tu tarea del documento)")
print("="*50)

import re
import nltk
from nltk.corpus import stopwords

# Descargar stopwords
nltk.download('stopwords')
stop_words_es = set(stopwords.words('spanish'))
stop_words_en = set(stopwords.words('english'))

print(" Stopwords descargadas")

def limpiar_texto(texto):
    """
    Aplica todas las limpiezas del documento:
    1. Lowercase
    2. Eliminar URLs
    3. Eliminar emojis/menciones/hashtags
    4. Eliminar stopwords
    5. Correcci√≥n b√°sica de ruido
    """
    if not isinstance(texto, str):
        return ""

    # 1. Min√∫sculas
    texto = texto.lower()

    # 2. Eliminar URLs
    texto = re.sub(r'https?://\S+|www\.\S+', ' ', texto)

    # 3. Eliminar menciones (@usuario) y hashtags
    texto = re.sub(r'@\w+', ' ', texto)
    texto = re.sub(r'#\w+', ' ', texto)

    # 4. Eliminar emojis
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags
        "]+", flags=re.UNICODE)
    texto = emoji_pattern.sub(r' ', texto)

    # 5. Eliminar caracteres especiales (mantener letras, n√∫meros, espacios b√°sicos)
    texto = re.sub(r'[^\w\s√°√©√≠√≥√∫√±√º]', ' ', texto)

    # 6. Eliminar n√∫meros (opcional - descomenta si quieres)
    # texto = re.sub(r'\d+', ' ', texto)

    # 7. Eliminar espacios m√∫ltiples
    texto = re.sub(r'\s+', ' ', texto).strip()

    # 8. Eliminar stopwords
    palabras = texto.split()
    palabras_filtradas = []
    for palabra in palabras:
        if palabra not in stop_words_es and palabra not in stop_words_en and len(palabra) > 2:
            palabras_filtradas.append(palabra)

    texto = ' '.join(palabras_filtradas)

    return texto

# Aplicar limpieza a una muestra primero (para probar)
print("\nüîß Aplicando limpieza de texto...")
print("   (Esto puede tomar varios minutos para 115k filas)")

# Aplicar a TODO el dataset
df['text_clean'] = df['text'].apply(limpiar_texto)

print(" Texto limpiado")

# 7. ELIMINAR TEXTOS DEMASIADO CORTOS
print("\n7.  Filtrando textos muy cortos...")
df['longitud'] = df['text_clean'].str.split().str.len()
df_filtrado = df[df['longitud'] >= 3].copy()

print(f"   Filas antes: {len(df):,}")
print(f"   Filas despu√©s (‚â•3 palabras): {len(df_filtrado):,}")
print(f"   Filas eliminadas: {len(df) - len(df_filtrado):,}")

# 8. GUARDAR DATASET LIMPIO
columnas_finales = ['text_clean', 'sentiment_num', 'source', 'text']  # 'text' original por si acaso
df_filtrado[columnas_finales].to_csv('/content/DATASET_LIMPIO_FINAL.csv', index=False)

print(f"\n Guardado: /content/DATASET_LIMPIO_FINAL.csv")

# 9. MOSTRAR EJEMPLOS ANTES/DESPU√âS
print(f"\n EJEMPLOS DE LIMPIEZA:")
ejemplos = df_filtrado.sample(5, random_state=42)
for i, row in ejemplos.iterrows():
    print(f"\n--- Ejemplo {i+1} ---")
    print(f"ORIGINAL: {str(row['text'])[:100]}...")
    print(f"LIMPIO:   {row['text_clean'][:100]}...")
    print(f"Sentimiento: {row['sentiment_num']} ({['neg','neu','pos'][row['sentiment_num']]})")
    print(f"Palabras: {row['longitud']}")

# PASO 7: AN√ÅLISIS DE DISTRIBUCI√ìN
print("\n" + "="*50)
print("AN√ÅLISIS DE DISTRIBUCI√ìN (para tu informe)")
print("="*50)

import matplotlib.pyplot as plt

# Configurar matplotlib
plt.style.use('seaborn-v0_8')

# 1. Distribuci√≥n de sentimientos
print("\n1. DISTRIBUCI√ìN DE CLASES:")
dist = df_filtrado['sentiment_num'].value_counts().sort_index()
for num, count in dist.items():
    etiqueta = ['Negativo', 'Neutro', 'Positivo'][num]
    porcentaje = (count / len(df_filtrado)) * 100
    print(f"   ‚Ä¢ {etiqueta} ({num}): {count:,} filas ({porcentaje:.1f}%)")

# Gr√°fico de distribuci√≥n
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Gr√°fico 1: Distribuci√≥n de sentimientos
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']  # rojo, turquesa, azul
bars1 = axes[0].bar(['Negativo', 'Neutro', 'Positivo'], dist.values, color=colors)
axes[0].set_title('Distribuci√≥n de Sentimientos', fontsize=14, fontweight='bold')
axes[0].set_ylabel('Cantidad de Filas', fontsize=12)

# A√±adir valores encima de las barras
for bar in bars1:
    height = bar.get_height()
    axes[0].text(bar.get_x() + bar.get_width()/2., height + 100,
                f'{int(height):,}', ha='center', va='bottom')

# 2. Distribuci√≥n por fuente
print("\n2. DISTRIBUCI√ìN POR FUENTE:")
dist_fuente = df_filtrado['source'].value_counts()
for fuente, count in dist_fuente.items():
    porcentaje = (count / len(df_filtrado)) * 100
    print(f"   ‚Ä¢ {fuente}: {count:,} filas ({porcentaje:.1f}%)")

# Gr√°fico 2: Distribuci√≥n por fuente
colors_fuente = ['#FF9AA2', '#FFB7B2', '#FFDAC1', '#E2F0CB']
bars2 = axes[1].bar(dist_fuente.index, dist_fuente.values, color=colors_fuente)
axes[1].set_title('Distribuci√≥n por Fuente de Datos', fontsize=14, fontweight='bold')
axes[1].set_ylabel('Cantidad de Filas', fontsize=12)
axes[1].tick_params(axis='x', rotation=45)

# A√±adir valores
for bar in bars2:
    height = bar.get_height()
    axes[1].text(bar.get_x() + bar.get_width()/2., height + 100,
                f'{int(height):,}', ha='center', va='bottom')

plt.tight_layout()
plt.savefig('/content/distribucion_clases.png', dpi=150, bbox_inches='tight')
print(f"\n Gr√°fico guardado: /content/distribucion_clases.png")
plt.show()

# 3. Estad√≠sticas de longitud de texto
print("\n3. ESTAD√çSTICAS DE LONGITUD DE TEXTO:")
print(f"   ‚Ä¢ Total filas: {len(df_filtrado):,}")
print(f"   ‚Ä¢ Palabras por texto (promedio): {df_filtrado['longitud'].mean():.1f}")
print(f"   ‚Ä¢ Palabras por texto (m√°x): {df_filtrado['longitud'].max():,}")
print(f"   ‚Ä¢ Palabras por texto (m√≠n): {df_filtrado['longitud'].min()}")

# 4. GUARDAR REPORTE FINAL
print(f"\n GUARDANDO REPORTE FINAL...")

with open('/content/REPORTE_PREPROCESAMIENTO.txt', 'w', encoding='utf-8') as f:
    f.write("="*60 + "\n")
    f.write("REPORTE DE PREPROCESAMIENTO - RUBEN GUANIN\n")
    f.write("="*60 + "\n\n")

    f.write("DATASET FINAL\n")
    f.write("-"*40 + "\n")
    f.write(f"Total filas: {len(df_filtrado):,}\n")
    f.write(f"Total columnas: {len(df_filtrado.columns)}\n")
    f.write(f"Columnas: {', '.join(df_filtrado.columns)}\n\n")

    f.write("DISTRIBUCI√ìN DE SENTIMIENTOS\n")
    f.write("-"*40 + "\n")
    for num, count in dist.items():
        etiqueta = ['Negativo', 'Neutro', 'Positivo'][num]
        porcentaje = (count / len(df_filtrado)) * 100
        f.write(f"{etiqueta} ({num}): {count:,} filas ({porcentaje:.1f}%)\n")

    f.write("\nDISTRIBUCI√ìN POR FUENTE\n")
    f.write("-"*40 + "\n")
    for fuente, count in dist_fuente.items():
        porcentaje = (count / len(df_filtrado)) * 100
        f.write(f"{fuente}: {count:,} filas ({porcentaje:.1f}%)\n")

    f.write("\nLIMPIEZA APLICADA\n")
    f.write("-"*40 + "\n")
    f.write("1. Lowercase (min√∫sculas)\n")
    f.write("2. Eliminaci√≥n de URLs\n")
    f.write("3. Eliminaci√≥n de menciones (@) y hashtags (#)\n")
    f.write("4. Eliminaci√≥n de emojis\n")
    f.write("5. Eliminaci√≥n de caracteres especiales\n")
    f.write("6. Eliminaci√≥n de stopwords (espa√±ol/ingl√©s)\n")
    f.write("7. Filtrado de textos < 3 palabras\n")

    f.write("\nENTREGABLES GENERADOS\n")
    f.write("-"*40 + "\n")
    f.write("1. DATASET_ESTANDARIZADO.csv - Dataset con sentimientos estandarizados\n")
    f.write("2. DATASET_LIMPIO_FINAL.csv - Dataset con texto limpiado\n")
    f.write("3. distribucion_clases.png - Gr√°fico de distribuci√≥n\n")
    f.write("4. REPORTE_PREPROCESAMIENTO.txt - Este reporte\n")

print(f" Reporte guardado: /content/REPORTE_PREPROCESAMIENTO.txt")
print(f"\n ¬°TAREA DE PREPROCESAMIENTO COMPLETADA!")

"""MODELADO Y ENTRENAMIENTO (DEEP LEARNING)"""

# ==============================================================================
# PARTE DE CRISTIAN BARAJA - MODELADO Y ENTRENAMIENTO (DEEP LEARNING)
# ==============================================================================
"""
ROL: Modelado y Entrenamiento Deep Learning
TAREAS:
1. Tokenizaci√≥n y Secuenciaci√≥n (Embeddings)
2. Arquitectura del Modelo (BiLSTM)
3. Entrenamiento con Early Stopping
4. Guardado de Pesos y Gr√°ficas
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

# Librer√≠as de Deep Learning (TensorFlow/Keras)
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, SpatialDropout1D
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split

print("üöÄ INICIANDO FASE DE MODELADO (Cristian Baraja)...")

# 1. CARGAR DATOS PROCESADOS POR RUBEN
# --------------------------------------------------------------------------
try:
    df = pd.read_csv('https://raw.githubusercontent.com/robincajas12/TRABAJOS_UNI/refs/heads/main/DATASET_LIMPIO_FINAL.csv')
    print(f"‚úÖ Dataset cargado: {len(df):,} filas")

    # Asegurarse que todos los textos sean strings (por seguridad)
    df['text_clean'] = df['text_clean'].astype(str)

except FileNotFoundError:
    print("‚ùå ERROR: No se encuentra 'DATASET_LIMPIO_FINAL.csv'. Ejecuta primero la parte de Rub√©n.")

# 2. PREPARACI√ìN DE DATOS PARA LA RED NEURONAL
# --------------------------------------------------------------------------
# Hiperpar√°metros de texto
MAX_NB_WORDS = 20000    # N√∫mero m√°ximo de palabras en el vocabulario
MAX_SEQUENCE_LENGTH = 100 # Longitud fija de cada tweet (padding)
EMBEDDING_DIM = 100     # Dimensi√≥n de los vectores de palabras

print("\n‚öôÔ∏è Tokenizando texto...")
tokenizer = Tokenizer(num_words=MAX_NB_WORDS, oov_token="<OOV>")
tokenizer.fit_on_texts(df['text_clean'].values)
word_index = tokenizer.word_index
print(f"   Vocabulario encontrado: {len(word_index):,} palabras")

# Convertir texto a secuencias num√©ricas
X = tokenizer.texts_to_sequences(df['text_clean'].values)
X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)
print(f"   Shape de datos de entrada (tensor): {X.shape}")

# Preparar etiquetas (One-Hot Encoding)
# 0: Negativo, 1: Neutro, 2: Positivo
Y = pd.get_dummies(df['sentiment_num']).values
print(f"   Shape de etiquetas: {Y.shape}")

# Divisi√≥n Train / Test (80% entrenamiento, 20% validaci√≥n)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)
print(f"   Train: {X_train.shape[0]:,} | Test: {X_test.shape[0]:,}")

# 3. DEFINICI√ìN DE LA ARQUITECTURA (BiLSTM)
# --------------------------------------------------------------------------
print("\nüèóÔ∏è Construyendo modelo BiLSTM...")

model = Sequential()

# Capa de Embedding (aprende representaci√≥n vectorial de palabras)
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))

# Dropout espacial para evitar overfitting en embeddings
model.add(SpatialDropout1D(0.2))

# Capa BiLSTM (El coraz√≥n del modelo)
# Bidirectional permite aprender contexto pasado y futuro
model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))

# Capas densas para clasificaci√≥n
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5)) # Apagar neuronas al azar para generalizar mejor
model.add(Dense(3, activation='softmax')) # 3 neuronas de salida (Neg, Neu, Pos)

# Compilaci√≥n (Loss: CrossEntropy, Optimizador: Adam) - Requisitos del PDF
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

# 4. ENTRENAMIENTO
# --------------------------------------------------------------------------
print("\nüèãÔ∏è‚Äç‚ôÇÔ∏è Iniciando entrenamiento...")

# Callbacks (Requisito: Early Stopping)
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
checkpoint = ModelCheckpoint('pesos_modelo_bilstm.h5', monitor='val_loss', save_best_only=True)

# Ajuste de hiperpar√°metros de entrenamiento
epochs = 10  # N√∫mero de vueltas (con early stopping probablemente pare antes)
batch_size = 64

history = model.fit(X_train, Y_train,
                    epochs=epochs,
                    batch_size=batch_size,
                    validation_split=0.1, # 10% de train para validaci√≥n interna
                    callbacks=[early_stopping, checkpoint])

# 5. RESULTADOS Y GR√ÅFICAS (Entregables)
# --------------------------------------------------------------------------
print("\nüìä Generando gr√°ficas de entrenamiento...")

plt.figure(figsize=(12, 4))

# Gr√°fica de Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', color='#4ECDC4')
plt.plot(history.history['val_accuracy'], label='Val Accuracy', color='#FF6B6B')
plt.title('Precisi√≥n del Modelo (Accuracy)')
plt.xlabel('√âpoca')
plt.ylabel('Accuracy')
plt.legend()

# Gr√°fica de Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', color='#4ECDC4')
plt.plot(history.history['val_loss'], label='Val Loss', color='#FF6B6B')
plt.title('P√©rdida del Modelo (Loss)')
plt.xlabel('√âpoca')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.savefig('graficas_entrenamiento.png')
plt.show()

# 6. GUARDAR ENTREGABLES
# --------------------------------------------------------------------------
# Guardar el modelo completo
model.save('modelo_final_sentiment.h5')

# Guardar el tokenizador (NECESARIO para la API de Santy)
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

print("\n‚úÖ ENTREGABLES GENERADOS (Cristian):")
print("   1. 'modelo_final_sentiment.h5' (El modelo entrenado)")
print("   2. 'pesos_modelo_bilstm.h5' (Solo los mejores pesos)")
print("   3. 'tokenizer.pickle' (Diccionario para traducir texto a n√∫meros)")
print("   4. 'graficas_entrenamiento.png' (Reporte visual)")

# ==============================================================================
# PARTE DE SANTY - EVALUACI√ìN Y PRODUCCI√ìN
# ==============================================================================
import pandas as pd
import numpy as np
import pickle
import re
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import classification_report, confusion_matrix

# Configuraci√≥n inicial
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words_es = set(stopwords.words('spanish'))
stop_words_en = set(stopwords.words('english'))

MAX_SEQUENCE_LENGTH = 100
categorias = {0: 'Negativo', 1: 'Neutro', 2: 'Positivo'}

print("üîÑ Cargando modelo y archivos de Santy...")
try:
    # Ajustamos el nombre al que aparece en tu captura de pantalla
    model = load_model('modelo_final_sentiment.h5')
    with open('tokenizer.pickle', 'rb') as handle:
        tokenizer = pickle.load(handle)
    df_eval = pd.read_csv('DATASET_LIMPIO_FINAL.csv')
    print("‚úÖ Todo listo: Modelo, Tokenizer y Dataset cargados correctamente.")
except Exception as e:
    print(f"‚ùå Error al cargar archivos: {e}. Verifica los nombres en el panel izquierdo.")

def limpiar_texto_santy(texto):
    if not isinstance(texto, str): return ""
    texto = texto.lower()
    # Eliminar URLs, menciones y hashtags seg√∫n el requerimiento del proyecto
    texto = re.sub(r'https?://\S+|www\.\S+', ' ', texto)
    texto = re.sub(r'@\w+|#\w+', ' ', texto)
    # Eliminar caracteres especiales y emojis
    texto = re.sub(r'[^\w\s√°√©√≠√≥√∫√±√º]', ' ', texto)
    texto = re.sub(r'\s+', ' ', texto).strip()
    # Filtrado de stopwords (espa√±ol e ingl√©s)
    palabras = [p for p in texto.split() if p not in stop_words_es and p not in stop_words_en and len(p) > 2]
    return ' '.join(palabras)

print("üß™ Generando m√©tricas de evaluaci√≥n...")

# Preparar datos de prueba desde el archivo CSV que subiste
X_eval = tokenizer.texts_to_sequences(df_eval['text_clean'].astype(str).values)
X_eval_pad = pad_sequences(X_eval, maxlen=MAX_SEQUENCE_LENGTH)
y_true = df_eval['sentiment_num'].values

# Predicciones del modelo
y_pred_probs = model.predict(X_eval_pad)
y_pred = np.argmax(y_pred_probs, axis=1)

# Reporte de Clasificaci√≥n (F1-score solicitado)
print("\nüìù REPORTE DE CLASIFICACI√ìN PROFESIONAL:")
print(classification_report(y_true, y_pred, target_names=['Negativo', 'Neutro', 'Positivo']))

# Gr√°fico de la Matriz de Confusi√≥n
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negativo', 'Neutro', 'Positivo'],
            yticklabels=['Negativo', 'Neutro', 'Positivo'])
plt.title('Matriz de Confusi√≥n - Evaluaci√≥n Santy')
plt.xlabel('Predicci√≥n')
plt.ylabel('Realidad')
plt.show()

!pip install -q gradio
import gradio as gr

def clasificador_sentimientos(texto):
    texto_p = limpiar_texto_santy(texto)
    seq = tokenizer.texts_to_sequences([texto_p])
    pad = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)
    pred = model.predict(pad)[0]
    idx = np.argmax(pred)
    confianza = pred[idx]

    return f"Sentimiento: {categorias[idx]}\nConfianza: {confianza:.2%}"

# Configuraci√≥n de la interfaz de usuario
print("üåê Lanzando interfaz de producci√≥n...")
app = gr.Interface(
    fn=clasificador_sentimientos,
    inputs=gr.Textbox(lines=2, placeholder="Ej: Me encanta este proyecto de IA"),
    outputs="text",
    title="An√°lisis de Sentimientos BiLSTM - Santy",
    description="Escribe un comentario y el modelo clasificar√° el sentimiento como Positivo, Neutro o Negativo."
)

app.launch(share=True)

# ==============================================================================
# AN√ÅLISIS DE ERRORES - SANTY
# ==============================================================================
import pandas as pd

def analizar_fallos(df_eval, y_true, y_pred, n_ejemplos=10):
    # Crear un DataFrame comparativo
    df_resultados = pd.DataFrame({
        'Texto_Original': df_eval['text'].values,
        'Texto_Limpio': df_eval['text_clean'].values,
        'Realidad': y_true,
        'Prediccion': y_pred
    })

    # Filtrar solo los casos donde hubo error
    df_errores = df_resultados[df_resultados['Realidad'] != df_resultados['Prediccion']].copy()

    # Mapear n√∫meros a etiquetas de texto para mejor lectura
    mapping = {0: 'Negativo', 1: 'Neutro', 2: 'Positivo'}
    df_errores['Realidad_Etiqueta'] = df_errores['Realidad'].map(mapping)
    df_errores['Prediccion_Etiqueta'] = df_errores['Prediccion'].map(mapping)

    print(f"‚ùå Total de errores encontrados en la muestra: {len(df_errores)}")
    print(f"Muestra de los primeros {n_ejemplos} fallos:\n")

    # Mostrar ejemplos espec√≠ficos
    for i, row in df_errores.head(n_ejemplos).iterrows():
        print(f"--- Error #{i} ---")
        print(f"TEXTO: {row['Texto_Original'][:150]}...")
        print(f"üî¥ REALIDAD: {row['Realidad_Etiqueta']}")
        print(f"ü§ñ PREDICCI√ìN: {row['Prediccion_Etiqueta']}")
        print("-" * 30)

    return df_errores

# Ejecutar la funci√≥n
df_fallos = analizar_fallos(df_eval, y_true, y_pred)

# ==============================================================================
# BLOQUE DE RESPALDO AUTOM√ÅTICO - SANTY
# ==============================================================================
from google.colab import files

# 1. Guardar la Matriz de Confusi√≥n como imagen de alta calidad
plt.figure(figsize=(8,6))
sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negativo', 'Neutro', 'Positivo'],
            yticklabels=['Negativo', 'Neutro', 'Positivo'])
plt.title('Matriz de Confusi√≥n - Proyecto Santy')
plt.savefig('matriz_santy_final.png', dpi=300, bbox_inches='tight')
print("‚úÖ Imagen de matriz generada.")

# 2. Guardar el Reporte de M√©tricas en un archivo de texto
reporte_texto = classification_report(y_true, y_pred, target_names=['Negativo', 'Neutro', 'Positivo'])
with open('reporte_metricas_santy.txt', 'w') as f:
    f.write("REPORTE DE EVALUACI√ìN - PROYECTO IA\n")
    f.write("Responsable: Santy\n")
    f.write("="*40 + "\n")
    f.write(reporte_texto)
print("‚úÖ Reporte de texto generado.")

# 3. Descarga autom√°tica a tu PC
print("üì• Iniciando descargas...")
files.download('matriz_santy_final.png')
files.download('reporte_metricas_santy.txt')